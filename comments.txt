*********************************************************************
Project created by Zufang Wang  and HaoHai Liu 2018 Semster 1 for AI 
*********************************************************************
The project contains the following files:
Board Class
Player Class
Greedy Agents 1-5
Human Agent
Mirror Agent
Monte_Carlo_Algorithim (unfinished) 

For this project please mark our Mirror Agent, justifications given below.



*********************************************************************
Monte_Carlo_Tree_Search
*********************************************************************
There was an attempt to integrate the Monte Carlo Tree Search  into a greedy agent, however the issue was that it was quite difficult to get the agent itself functional and efficient enough to be kept under the required timeframe. Therefore work on the system stopped with the final code being disfunctional for now. 

The reason why Monte Carlo Tree Search was attempted was that it can technically resolve the branching factor problem while 
still giving a relatively optimal solution to the problem. This in turn made is a viable algorithim as there is a limit on
time and space imposed during this project. What was not known at the time of idation was that the simulaiton algorithim
which is based upon the greedy agents was too slow. Which in turn stalled the idea.


The Monte Carlo Tree Search works in the following way:
1. Selection
2. Expansion
3. Simulation
4. Back propergation

The selection phase typically involves walking down the most favourable path until you're at a leaf, at which an expansion phase will occur, this could be random or predetermined based upon a mathematical function, but for our case it could just be simply all the moves in which the agent could take. 

Simulation is then played out randomly on a set of nodes, the general method is to do the playouts randomly but we attempted to use greedy agent for this scenario. After much consideration it was estimated that even if we do 5 playouts of around 100 turns each, the speed of the Simulation part would always slow, which in turn threatens the simualtion time limit of the game. The simulation should always return a win or loss, which is problematic since some greedy search games will always get stuck in a loop. 

Back Propergation is essentially going back to the root to the tree and updating the number of wins and loses within a simulation. 



