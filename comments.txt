*********************************************************************
Project B by Zufeng Wang and HaoHai Liu 2018 Semester 1 AI COMP30024
*********************************************************************
The project contains the following Python files:
board.py
random-agent.py
greedy-agent.py
greedy-agent-2.py
greedy-agent-3.py
greedy-agent-4.py
greedy-agent-5.py
human-agent.py
mirror-agent.py
Monte_Carlo_Algorithm.py (unfinished)

For this project please mark our mirror-agent, justifications given below.
	(the only two files required for it to work are board.py and mirror-agent.py)
	(the Player class is in mirror-agent.py)

The overall structure of the project is one Board class (in the board module), and multiple agents that extend and use methods from it. There is a lot of duplicate code in the agent files, but that is an intentional decision to allow for any number of agents to be deleted without breaking dependencies in the others.

The agents are the result of a number of design iterations. Here is a summary:

random-agent
	This was created along with the board as the first step to make the simulation work. It plays random valid actions. Because of this, it is the fastest but also the weakest agent.

greedy-agent
	The first of the hill-climbing agents. It calculates the next board state following each of its possible actions. Then it plays an action resulting the most favourable state. To determine how favourable states are, it has a basic valuation function that considers the number of pieces remaining in each team. Because of this, it will capture opponent pieces (greedily) when it can.

greedy-agent-2
	Same as greedy agent, except its valuation function also considers the distance of pieces from the centre. This results in pieces trying to clump in the centre of the board.

greedy-agent-3
	Like greedy-agent-2, but the valuation function also considers which pieces are threatened during Placing Phase. (A major weakness of all the previous agents was that they sometimes places pieces in positions where they could be captured by the opponent on the next turn.) It also takes into account the turn (i.e. player initiative): If a team plays an action and its pieces are threatened, it is considered worse than if it played an action and the opponent pieces were threatened. This is because the opponent has initiative after this action and can choose to capture and/or defend.

greedy-agent-4
	Like greedy-agent-3, but does Minimax to search more turns deep. (The previous greedy-agents had search depth of 1.) It uses different search depths depending on the phase of the game. Later phases tend to have lower branching factor.
	It also has an Opening Book containing pre-computed good first moves for WHITE. (These moves were calculated using our domain knowledge as well as deeper Minimax searches.)

greedy-agent-5
	Like greedy-agent-4, but also does alpha-beta pruning. (The algorithm used is based on the pseudocode from Wikipedia (https://en.wikipedia.org/wiki/Alpha-beta_pruning).) It runs quicker due to the pruning.

mirror-agent
	Like greedy-agent-5, but also implements a Mirror strategy: When playing as BLACK, the mirror-agent may choose to mirror WHITE’s moves during Placing Phase. This tactic was implemented in an attempt to combat WHITE’s first turn advantage. WHITE’s first turn advantage usually allows it to gain control of the central four squares, which are important in the end game, if playing optimally. However, Mirror strategy allows BLACK to maintain an equivalent position to WHITE except one move behind. Black can also stop Mirroring early if it thinks there’s a better move (e.g. if playing against a weaker agent).
	The alpha-beta pruning is optimised slightly by sorting the actions list (by distance to centre of board) during Placing Phase. This helps it to prune branches containing lower-value moves earlier. Mirror-agent searches one turn deeper during Placing Phase compared to greedy-agent-5 due to the importance of that phase (and the time savings afforded by creative optimisations).
	It also has an extra part in the valuation function that considers win conditions.
	All these features combined make mirror-agent our strongest agent overall.

human-agent
	This agent was created to play using user input for testing purposes.

*********************************************************************
Monte_Carlo_Tree_Search
*********************************************************************
There was an attempt to integrate the Monte Carlo Tree Search into a greedy agent, however the issue was that it was quite difficult to get the agent itself functional and efficient enough to be kept under the required timeframe. Therefore work on the system stopped with the final code being dysfunctional for now. 

The reason why Monte Carlo Tree Search was attempted was that it can technically resolve the branching factor problem while 
still giving a relatively optimal solution to the problem. This in turn made it a viable algorithm as there is a limit on
time and space imposed during this project. What was not known at the time of ideation was that the simulation algorithm
which is based upon the greedy agents was too slow. Which in turn stalled the idea.


The Monte Carlo Tree Search works in the following way:
1. Selection
2. Expansion
3. Simulation
4. Back propagation

The selection phase typically involves walking down the most favourable path until you're at a leaf, at which an expansion phase will occur, this could be random or predetermined based upon a mathematical function, but for our case it could just be simply all the moves in which the agent could take. 

Simulation is then played out randomly on a set of nodes, the general method is to do the playouts randomly but we attempted to use greedy agent for this scenario. After much consideration it was estimated that even if we do 5 playouts of around 100 turns each, the speed of the Simulation part would always slow, which in turn threatens the simulation time limit of the game. The simulation should always return a win or loss, which is problematic since some greedy search games will always get stuck in a loop. 

Back Propagation is essentially going back to the root to the tree and updating the number of wins and loses within a simulation. 



